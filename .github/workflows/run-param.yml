name: Run Scraper with Parameter

concurrency:
  group: main-workflow-group
  cancel-in-progress: false

on:
  workflow_dispatch:
    inputs:
      api_param:
        description: 'Search query from Vercel'
        required: true
        type: string

permissions:
  contents: write

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 playwright requests

      - name: Install Playwright browsers
        run: |
          python -m playwright install chromium
          python -m playwright install-deps

      - name: Create data directory
        run: |
          mkdir -p data

      - name: Read Counter
        id: read_count
        run: |
          if [ ! -f counter.json ]; then
            echo "Creating initial counter.json"
            echo '{"count": 10}' > counter.json
          fi
          COUNT=$(python -c "import json; print(json.load(open('counter.json'))['count'])")
          echo "current_count=$COUNT" >> $GITHUB_OUTPUT
          echo "Current limit remaining: $COUNT"

      - name: Check Limit
        run: |
          if [ ${{ steps.read_count.outputs.current_count }} -le 0 ]; then
            echo "ðŸš« Limit reached. Stopping workflow."
            echo "Counter is zero or negative. Please reset counter.json."
            exit 1
          fi

      - name: Debug - Show Input Parameter
        run: |
          echo "Received query parameter: ${{ github.event.inputs.api_param }}"
          # Use Python to get length safely
          python -c "query = '${{ github.event.inputs.api_param }}'; print(f'Parameter length: {len(query)}')"

      # ----------------- RUN PYTHON SCRIPT -----------------
      - name: Run Python Scraper
        id: scrape
        run: |
          # Set timeout for the scraper (10 minutes max)
          timeout 600 python scraper.py "${{ github.event.inputs.api_param }}"
          
          # Check if any data was generated
          if [ -d "data" ]; then
            echo "Data directory contains:"
            ls -la data/
            
            # Count number of results files
            FILE_COUNT=$(find data -name "*.json" -type f | wc -l)
            echo "files_generated=$FILE_COUNT" >> $GITHUB_OUTPUT
          else
            echo "files_generated=0" >> $GITHUB_OUTPUT
          fi
          
        env:
          MAX_PAGES: 5
          PYTHONUNBUFFERED: 1
          
      # ----------------------------------------------------

      - name: Check Scraper Results
        if: steps.scrape.outputs.files_generated == 0
        run: |
          echo "âš ï¸ Warning: No data files were generated by the scraper."
          echo "This might be due to:"
          echo "1. No results found for the query"
          echo "2. Google blocking the request"
          echo "3. Scraper timeout or error"

      - name: Decrement Counter
        run: |
          CURRENT_COUNT=$(python -c "import json; print(json.load(open('counter.json'))['count'])")
          NEW_COUNT=$((CURRENT_COUNT - 1))
          echo "Old count: $CURRENT_COUNT, New count: $NEW_COUNT"
          python -c "import json; data = {'count': $NEW_COUNT}; json.dump(data, open('counter.json', 'w'), indent=2)"
          
      - name: Set up Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
      - name: Commit Changes
        id: commit
        run: |
          # Add counter file
          git add counter.json
          
          # Add data directory if it has files
          if [ -d "data" ] && [ "$(ls -A data)" ]; then
            git add data/
          fi
          
          # Check if there are any changes to commit
          if git diff --cached --quiet; then
            echo "No changes to commit"
            echo "changes_committed=false" >> $GITHUB_OUTPUT
          else
            git commit -m "Update scraper results and counter [skip ci]

            Query: ${{ github.event.inputs.api_param }}
            Files generated: ${{ steps.scrape.outputs.files_generated }}"
            echo "changes_committed=true" >> $GITHUB_OUTPUT
          fi
          
      - name: Push Changes
        if: steps.commit.outputs.changes_committed == 'true'
        run: |
          git push

      - name: Create Summary Report
        if: always()
        run: |
          echo "## Scraper Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Query:** \`${{ github.event.inputs.api_param }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Read current count
          if [ -f "counter.json" ]; then
            CURRENT_COUNT=$(python -c "import json; print(json.load(open('counter.json'))['count'])")
            echo "**Counter:** $CURRENT_COUNT remaining" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # List generated files
          if [ -d "data" ] && [ "$(ls -A data)" ]; then
            echo "**Data Generated:**" >> $GITHUB_STEP_SUMMARY
            for file in data/*.json; do
              if [ -f "$file" ]; then
                FILE_NAME=$(basename "$file")
                echo "- \`$FILE_NAME\`" >> $GITHUB_STEP_SUMMARY
              fi
            done
          else
            echo "**No data files generated**" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Status
          if [ "${{ job.status }}" = "success" ]; then
            echo "âœ… **Workflow completed successfully**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Workflow failed or was cancelled**" >> $GITHUB_STEP_SUMMARY
          fi
